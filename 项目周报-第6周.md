# 项目周报

日期：2025-05-10

项目实践题目：文本的匹配与推荐

## 实践内容
完成“支线任务10”
阅读Gao 2023
### 感受
根据指引选择了Qwen/Qwen3-8B这个模型，并注册创建了api  
创建文件llm-test后将参考文档中的示例代码进行了适当的修改  
在电脑上运行后成功出现结果 
#### 阅读收获  
**密集检索**:  
是一种利用 语义嵌入相似性检索文档的方法，已被证明 ，在网络搜索、问题解答和事实验证等任务中取得了成功。  
各种 方法，如负挖掘、蒸馏、特定检索预训练、语义内嵌相似性检索，以及缩放 ，以提高 有监督密集检索模型的有效性。  
替代迁移学习设置：在高资源数据集上训练密集 检索器，然后在不同领域的查询上对其进行评估。MS MARCO是最常用的一种方法，它是一个具有 大量人工判断查询-文档 对的数据集，但Izacard 等人认为，在实践中，不能总是假定存在这样一个大型数据集 。此外，MS MARCO 限制了商业用途，无法在各种 真实世界的搜索场景中采用。  
**自监督表示学习方法**:  
现代深度学习有两种不同的方法。  
在标记层面，在大型语料库上预先训练的生成式大型语言模型（LLMs）已经证明了强大的自然语言 理解（NLU）和生成（NLG）能力。  
在文档层面，使用 对比目标预先训练的文本（块）编码器学会将文档与文档的相似性编码为内积。  
**额外见解**：  
经过进一步训练以遵从指令的 LLMs 可以对各种未见的 指令进行零点泛化。其中，InstructGPT 表明，只需少量数据，GPT-3模型就能对准 ，使其忠实地遵循指令的人类意图。  
**通过假设文档嵌入为支点，将密集检索分解为两项任务**:  
由指令跟随语言模型执行的生成任务和由对比编码器执行的文档相似性任务  
首先，将查询输入生成模型，并指示它 "写一份能回答问题的文档"，即 一个假设文档。希望生成过程通过提供一个示例来捕捉 "相关性"。生成的文档不是真实的，可能包含事实错误，但 "像 "相关文档。  
第二步，使用无监督对比编码器将该文档编码为嵌入向量。在这里，希望编码器的密集瓶颈能充当有损压缩器，将额外的（幻觉的）细节从嵌入向量中过滤掉。使用这个向量 来搜索语料库的嵌入向量。最相似的真实文档被检索并返回。检索利用的是在对比预训练阶段学习到的内积中编码的文档-文档相似性。  
在Gao他们提出的 HyDE 因式分解中，查询-文档相似性的得分不再明确建模或计算。取而代之的是，检索任务被分为两个任务（NLU 和 NLG），构建 HyDE 中不需要监督，也不需要训练新的模型。生成模型和对比性 编码器都是 "开箱即用"，无需任何调整或修改。同时，使用 InstructGPT和 Contriever的 HyDE "原样 "明显优于之前最先进的 Contriever 纯零镜头模型  
**自我监督学习**：  
掩码语言模型，如 BERT，在表示文本方面展示了强大的能力。  
而拥有千亿级参数的大型语言模型（LLMs）在各种任务中的少镜头和零镜头设置下表现出了卓越的泛化能力。  
尽管它们取得了广泛的成功，但零次或少次学习很少被直接用于排名，唯一的例外是 Sachan et al，该研究进行了零次重新排序。    
除了语言建模，对比学习方法有助于神经语言模型学习如何将文本块（如句子或段落）表示为嵌入向量。  
无需任何监督，这种对比编码器可以将同质文本块嵌入到一个向量空间中。而且其中一些距离函数（如内积）可以捕捉相似性。  
**指令跟随模型**：  
大语言模型出现后不久，一些研究小组发现，根据指令及其执行数据训练的 LLM泛化，以执行带有新指令的新任务，这可以使用标准的监督序列-序列学习技术来实现，也可以更有效地通过人类反馈的强化学习来实现  
**带有指令的任务感知检索**：  
对密集编码器进行了微调，使其能够也能对任务特定指令进行编码查询。  
与此相比，指令跟随使用的是无监督编码器，使用生成式LLM 处理不同的任务，而无需进行任何微调。    
**稠密检索**:  
稠密向量空间中的文档检索，在预训练 Transformer 语言模型出现后，人们对其进行了广泛研究。 研究人员研究了度量学习问题，如训练 损失和负采样等度量学习问题进行了研究，还引入了蒸馏法。后来的研究 语言模型的第二阶段预训练 的第二阶段预训练以及模型缩放。所有这些方法都依赖于有监督的对比学习。  
**零点密集检索**:  
Thakur 等人的研究从经验上突出了零镜头（密集）检索任务在神经检索界的地位。  
他们的 BEIR 基准包括 各种检索任务。该论文和许多后续研究 研究考虑了迁移学习设置 在这种情况下，密集检索器首先使用一个 训练，即 MS MARCO.  

