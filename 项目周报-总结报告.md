# 项目总结报告

日期：2025-06-08

项目实践题目：文本的匹配与推荐  

## 内容  
在Gao 2023的论文中，作者提出了一个零样本检索的新方法：HyDE方法,这种方法是将我们的查询映射为“假设文档”，这个“假设文档”包含了我们查询的相关语义（即我们查询的prompt提示词和相关的术语等），然后将查询与假设文档编码为向量并求平均，得到 HyDE 向量，再与真实文档向量进行余弦相似度检索,这点能够较好的解决这个项目的主要问题，即如何根据prompt从海量文献中找到匹配的论文,进行余弦相似度检索能够更好地匹配相关文章和我们给出的prompt提示词，从而能够更好地使其推荐给我们想要的相关文章。    
而同时，在翻阅作者Luyu Gao主页的论文时，发现作者还提出了几种方法，主动检索增强生成框架、FLARE方法、COIL信息检索架构还有Condenser架构（改良版），都是运用一种架构模型，通过文本或文字向量的相似度、编码器聚合的信息结合使得密集检索的性能提升完成对上下文的匹配。这里重点总结Condenser架构，这种架构基于 Transformer 编码器，通过在预训练阶段引入一个特殊的“Condenser Head”来优化模型结构，使其更适合于密集编码器的任务，它通过在早期和晚期编码器层之间建立一个短路连接，使得Condenser 强制模型在预训练阶段就关注于全局信息的聚合，而不是仅仅依赖于局部信息。而且作者在第二篇关于Condenser的论文中，更是提出了coCondenser架构（基于 Condenser 架构，通过无监督的语料级对比损失来预训练密集检索模型），它相比Condenser多加了一个对比损失函数，使得来自同一文档的片段的 CLS 嵌入接近，而来自不同文档的片段的 CLS 嵌入远离。并且还采用了梯度缓存技术，将表示梯度和编码器梯度计算解耦，从而避免了大批次数据同时占用 GPU 内存，使得密集检索的性能得到了进一步的提升。  
HyDE方法和Condenser架构这两种方法都是无监督的，但HyDE方法仅需要LLM生成，而Condenser则依赖大规模语料预训练,HyDE方法能应用于跨语言、临时主题检索，而Condenser则适用于大规模静态文档库的高效语义匹配,并且HyDe生成延迟高，成本随查询量线性增长,Condenser架构需大量计算资源，灵活性较弱。  
关于文献调查:  
主要是先了解项目需求是什么，再根据需求确定项目需要使用什么方法，然后查找相关技术，然后根据查找到的论文，看论文作者的主页（一般都会有几篇相关技术的文章），然后粗略阅读一下了解每篇文章使用什么方法，优缺点是什么。  
除了提及的HyDE方法其他方法来自: 

[^FLARE方法](https://arxiv.org/pdf/2305.06983)   

[^coCondenser](https://arxiv.org/pdf/2108.05540)  

[^Condenser架构](https://arxiv.org/pdf/2104.08253)   

[^COIL](https://arxiv.org/pdf/2104.07186)    

### HyDE方法  
HyDE方法主要是“假设文档生成”，这需要利用预训练生成模型（instructGPT和Contriever模型变体）,根据查询词生成 “可能相关的文档片段”，并将这些片段的嵌入作为 “假设文档嵌入”,并且这些嵌入能模拟真实相关文档的语义特征，从而引导检索系统更精准地匹配目标文档。并且在将查询嵌入与假设文档嵌入结合，还要调整检索模型的匹配策略扩展检索范围。其在网络搜索、低资源检索和多语言检索有着优于其他模型（Contriever 和BM25）的优势。    
在拥有这些优点的同时，HyDE也有许多不足：  
它的生成质量依赖预训练模型,如果生成模型对用户给的prompt语义理解不到位，可能导致假设文档嵌入与真实相关文档不匹配从而导致浪费资源。  
它每次查询需生成多个文档片段并编码，实时检索场景下可能影响响应速度。  
改进方向：  
结合轻量级生成模型（如蒸馏后的 GPT 模型）或稀疏检索策略降低计算开销。  
引入验证机制：通过后处理步骤（如关键词过滤、事实核查）筛选高质量假设文档，减少语义偏差。  
